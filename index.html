<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .profile {
      border-radius: 50%;
    }

    .one {
      width: 160px;
      height: 160px;
      max-width: 100%;
      max-height: 100%;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
  <title> Nithin Rao</title>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114832734-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-114832734-1');
</script>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name> Nithin Rao Koluguri</name>
              </p>
              <p> As a Senior Research Scientist &#64; <a href="https://developer.nvidia.com/conversational-ai">  NVIDIA Conversational AI,</a> I work on developing speech recogntion, speaker recognition, verification and diarization systems. My research interests include speech signal processing, natural language processing and machine learning.</p> 
              <p>
                I received my masters degree from University of Southern California (USC) with a major in Electrical and Computer Engineering. During my masters I carried out research through <a href="https://sail.usc.edu/">Signal Analysis and Interpretation Laboratory (SAIL)</a> where I was advised by <a href="https://sail.usc.edu/people/shri.html">Prof. Shrikanth Narayanan</a>.
	      </p>
              <p align=center>
                <a href="mailto:nithinrao.koluguri@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1SivSTm7iDPuI51_hierJan0wdScMmnzk/view">CV</a> &nbsp/&nbsp
                <a href="https://www.github.com/nithinraok">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/nithinraok/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=YPtcUTUAAAAJ&hl=en"> Google Scholar  </a> &nbsp/&nbsp
                <a href="https://medium.com/@nithinraok_">Blog</a>
              </p>
            </td>
            <td width="33%">
              <img class="profile" src="images/nithin_new.png">
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
                <heading style="color:rgb(244, 189, 184);" >  Prior Work Experience </heading>
              <p>
                <papertitle> Applied Machine Learning Intern at <a href="https://www.bose.com/en_us/index.html" > Bose CE Applied Research Group</a> </papertitle>
                <!-- <br> -->
                <br>
                <ul>
                  <li>Developed and Deployed NLP & Computer Vision related ML system on Google pixel phone and Bose wearables</li>
                </ul>
                
              </p>
              
              <p>
                <papertitle> Research Assistant at <a href="https://spire.ee.iisc.ac.in/" > SPIRE LAB, IISc Bangalore </a> (Advised by: <a href="http://www.ee.iisc.ac.in/new/people/faculty/prasantg/">Prof. Prasanta Kumar Ghosh</a>) </papertitle>
                <!-- <br> -->
                <br>
                <ul>
                  <li>Built a speech classifier to detect Amyotrophic Lateral Sclerosis (ALS) and Parkinsons (PD) diseases based on voice as bio-marker</li>
                  <li>Developed an unsupervised system for robust bird sound detection using enhanced Multiple Window Savitzky-Golay (MWSG) spectrogram. </li>
                </ul>
              </p>
            
              <p>
                <papertitle> Software Developer at <a href="https://spire.ee.iisc.ac.in/" > Robert Bosch Bangalore </a> </papertitle>
                <!-- <br> -->
                <br>
                <ul>
                  <li>Customer and Production diagnosis in Telematics Projects, where designed new features for Daimler customer.</li>
                  <li>Text to speech (TTS) outputs for various car multimedia features like navigation, SMS readout and hands free control and tested the output using TTFIS tool</li>
                </ul>
              </p>

            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading style="color:rgb(244, 189, 184);" >Research</heading>
              <p>
		I am interested in understanding signal level properties of audio,speech and Image. My research experiences thus far delve in natural language processing and applying machine learning algorithms on speech & image.
	      </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!-- Krishna C Puvvada, \textbf{Nithin Rao Koluguri}, Kunal Dhawan, Jagadeesh Balam, Boris Ginsburg ``Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition" \textit{IEEE ICASSP 2024}.Paper: https://arxiv.org/pdf/2309.10922 -->
            <tr>
                <td valign="top" width="75%">
                    <a href="https://arxiv.org/pdf/2309.10922">
                    <papertitle>Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition</papertitle>
                    </a>
                    <br>Krishna C Puvvada, <strong>Nithin Rao Koluguri</strong>, Kunal Dhawan, Jagadeesh Balam, Boris Ginsburg
                    <br>
                    <br>
                    <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024</em>
                    <br>
                    <a href="https://github.com/NVIDIA/NeMo">project page</a>
                    <p></p>
                    <p> We evaluate various discrete audio representations that can be used as an alternative to mel-spectrograms for speech and speaker recognition. </p>
                </td>
            </tr>

            <!-- \textbf{Nithin Rao Koluguri}, Samuel Kriman, Georgy Zelenfroind, Somshubra Majumdar, Dima Rekesh, Vahid Noroozi, Jagadeesh Balam, Boris Ginsburg ``Investigating End-to-End ASR Architectures for Long Form Audio Transcription" \textit{IEEE ICASSP 2024}.Paper: https://arxiv.org/pdf/2309.09950 -->
            <tr>
                <td valign="top" width="75%">
                    <a href="https://arxiv.org/pdf/2309.09950">
                    <papertitle>Investigating End-to-End ASR Architectures for Long Form Audio Transcription</papertitle>
                    </a>
                    <br><strong>Nithin Rao Koluguri</strong>, Samuel Kriman, Georgy Zelenfroind, Somshubra Majumdar, Dima Rekesh, Vahid Noroozi, Jagadeesh Balam, Boris Ginsburg
                    <br>
                    <br>
                    <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024</em>
                    <br>
                    <a href="https://github.com/NVIDIA/NeMo">project page</a>
                    <p></p>
                    <p> We investigate end-to-end ASR architectures for long form audio transcription, that can do inference in one single pass. </p>
                </td>
            </tr>

            <!-- Tae Jin Park, Kunal Dhawan, \textbf{Nithin Koluguri}, Jagadeesh Balam ``Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach" \textit{IEEE ICASSP 2024} Paper:https://arxiv.org/pdf/2309.05248  -->
            <tr>
                <td valign="top" width="75%">
                    <a href="https://arxiv.org/pdf/2309.05248">
                    <papertitle>Enhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach</papertitle>
                    </a>
                    <br>Tae Jin Park, Kunal Dhawan, <strong>Nithin Koluguri</strong>, Jagadeesh Balam
                    <br>
                    <br>
                    <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024</em>
                    <br>
                    <a href="https://github.com/NVIDIA/NeMo">project page</a>
                    <p></p>
                    <p> We propose a novel contextual beam search approach for speaker diarization that leverages large language models to improve speaker diarization performance. </p>
                </td>
            </tr>

            <!-- Dima Rekesh,\textbf{ Nithin Rao Koluguri}, Samuel Kriman, Somshubra Majumdar, Vahid Noroozi, He Huang, Oleksii Hrinchuk, Krishna Puvvada, Ankur Kumar, Jagadeesh Balam, Boris Ginsburg ``Fast conformer with linearly scalable attention for efficient speech recognition" \textit{IEEE ASRU 2023}. Paper: https://ieeexplore.ieee.org/iel7/10388490/10389614/10389701.pdf  -->
            <tr>
                <td valign="top" width="75%">
                    <a href="https://ieeexplore.ieee.org/iel7/10388490/10389614/10389701.pdf">
                    <papertitle>Fast conformer with linearly scalable attention for efficient speech recognition</papertitle>
                    </a>
                    <br>Dima Rekesh, <strong>Nithin Rao Koluguri</strong>, Samuel Kriman, Somshubra Majumdar, Vahid Noroozi, He Huang, Oleksii Hrinchuk, Krishna Puvvada, Ankur Kumar, Jagadeesh Balam, Boris Ginsburg
                    <br>
                    <br>
                    <em>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023</em>
                    <br>
                    <a href="https://github.com/NVIDIA/NeMo">project page</a>
                    <p></p>
                    <p> We propose a novel attention mechanism for Conformer architecture that is linearly scalable with respect to the input sequence length. </p>
                </td>
            </tr>
        <!-- Sanchit and Hugging Face Team and Nvidia NeMo Team and SpeechBrain Team Vaibhav Srivastav and Somshubra Majumdar, and \textbf{Nithin Koluguri }and Adel. ``Open Automatic Speech Recognition Leaderboard" Link:https://huggingface.co/spaces/hf-audio/open_asr_leaderboard  -->
            <tr>
                <td valign="top" width="75%">
                <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">
                    <papertitle>Open Automatic Speech Recognition Leaderboard</papertitle>
                </a>
                <br>
                Sanchit, Hugging Face Team, Nvidia NeMo Team, SpeechBrain Team, Vaibhav Srivastav, Somshubra Majumdar, <strong>Nithin Koluguri</strong>, Adel
                <br>
                <a href="https://github.com/huggingface/open_asr_leaderboard">project page</a>
                <p></p>
                <p> We present the Open Automatic Speech Recognition Leaderboard, a platform for benchmarking and comparing ASR models. </p>
                </td>
            </tr>

          <!-- For Fei Jia, \textbf{Nithin Rao Koluguri}, Jagadeesh Balam, and Boris Ginsburg. ``A Compact End-to-End Model with Local and Global Context for Spoken Language Identification." \textit{IEEE Interspeech 2023}. Paper:https://arxiv.org/pdf/2210.15781. Matter:  AmberNet, a compact end-to-end neural network for Spoken Language Identification. AmberNet consists of 1D depth-wise separable convolutions and Squeeze-and-Excitation layers with global context, followed by statistics pooling and linear layers. -->
            <tr>
                <td valign="top" width="75%">
                    <a href="https://arxiv.org/pdf/2210.15781">
                    <papertitle>A Compact End-to-End Model with Local and Global Context for Spoken Language Identification</papertitle>
                    </a>
                    <br>Fei Jia, <strong>Nithin Rao Koluguri</strong>, Jagadeesh Balam, Boris Ginsburg
                    <br>
                    <br>
                    <em>International Speech Communication Association (Interspeech), 2023</em>
                    <br>
                    <a href="https://github.com/NVIDIA/NeMo">project page</a>
                    <p></p>
                    <p> AmberNet, a compact end-to-end neural network for Spoken Language Identification. AmberNet consists of 1D depth-wise separable convolutions and Squeeze-and-Excitation layers with global context, followed by statistics pooling and linear layers. </p>"
                </td>
            </tr>
            <!-- For Tae Jin Park and He Huang and Coleman Hooper and \textbf{Nithin Rao Koluguri} and Kunal Dhawan and Ante Jukić and Jagadeesh Balam and Boris Ginsburg ``Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling Technique for Synthetic Data Generation" \textit{IEEE Interspeech 2023}. Paper:https://arxiv.org/pdf/2310.12371 -->
            <tr>
                <td valign="top" width="75%">
                    <a href="https://arxiv.org/pdf/2310.12371">
                    <papertitle>Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling Technique for Synthetic Data Generation</papertitle>
                    </a>
                    <br>Tae Jin Park, He Huang, Coleman Hooper, <strong>Nithin Rao Koluguri</strong>, Kunal Dhawan, Ante Jukić, Jagadeesh Balam, Boris Ginsburg
                    <br>
                    <br>
                    <em>International Speech Communication Association (Interspeech), 2023</em>
                    <br>
                    <a href="https://github.com/NVIDIA/NeMo">project page</a>
                    <p></p>
                    <p> We introduce a sophisticated multi-speaker speech data simulator, specifically engineered to generate multi-speaker speech recordings.</p>
                </td>
            </tr>
                              <!-- writing the tr code for paper  Tae Jin Park and He Huang and Ante Jukić and Kunal Dhawan and Krishna C. Puvvada and \textbf{Nithin Rao Koluguri} and Nikolay Karpov and Aleksandr Laptev and Jagadeesh Balam and Boris Ginsburg ``The CHiME-7 Challenge: System Description and Performance of NeMo Team’s DASR System" \textit{IEEE Interspeech 2023}. -->
          <tr>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2310.12378">
                <papertitle>The CHiME-7 Challenge: System Description and Performance of NeMo Team's DASR System </papertitle>    
                </a>
                <br>Tae Jin Park, He Huang, Ante Jukic, Kunal Dhawan, Krishna C Puvvada,<strong>Nithin Koluguri </strong> , Nikolay Karpov, Aleksandr Laptev, Jagadeesh Balam, Boris Ginsburg
                <br>
                <br>
                <em>International Speech Communication Association (Interspeech), 2023</em>
                <br>
                <a href="https://github.com/NVIDIA/NeMo">project page</a>
                <p></p>
                <p> We propose a novel multi-scale decoder for speech recognition, which is an ensembled NeMo diarization system.</p>
            </td>
          </tr>

          <tr>
            <!-- <td width="25%">
              <div class="one"><img src='images/speakernet.png'></div>
            </td> -->
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2203.15974">
                <papertitle>Multi-scale Speaker Diarization with Dynamic Scale Weighting</papertitle>
              </a>
              <br>
              Taejin Park, <strong> Nithin Rao Koluguri</strong>, Jagadeesh Balam, Boris Ginsburg
              <br>
              <br>
              <em>International Speech Communication Association (Interspeech), 2022 </em>
              <br>
              <a href="https://github.com/NVIDIA/NeMo">project page</a>
              <p></p>
              <p> Advanced multi-scale diarization system based on a multi-scale diarization decoder. </p>
            </td>
          </tr>

          <tr>
            <!-- <td width="25%">
              <div class="one"><img src='images/speakernet.png'></div>
            </td> -->
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2110.04410">
                <papertitle>TitaNet: Neural Model for speaker representation with 1D Depth-wise separable convolutions and global context</papertitle>
              </a>
              <br>
              <strong> Nithin Rao Koluguri</strong>,
              Taejin Park, Boris Ginsburg
              <br>
              <br>
              <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022</em>
              <br>
              <a href="https://github.com/NVIDIA/NeMo">project page</a>
              <p></p>
              <p>A novel neural network architecture for extracting speaker representations. Employs 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with global context followed by channel attention based statistics pooling layer to map variable-length utterances to a fixed-length embedding (t-vector). </p>
            </td>
          </tr>

          <tr>
            <!-- <td width="25%">
              <div class="one"><img src='images/speakernet.png'></div>
            </td> -->
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2010.12653">
                <papertitle>SpeakerNet: 1D Depth-wise Separable Convolutional Network for Text-Independent Speaker Recognition and Verification</papertitle>
              </a>
              <br>
              <strong> Nithin Rao Koluguri</strong>,
              Jason Li, Vitaly Lavrukhin, Boris Ginsburg
              <br>
              <br>
              <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021</em>
              <br>
              <a href="https://github.com/NVIDIA/NeMo">project page</a>
              <p></p>
              <p>We propose SpeakerNet - a new neural architecture for speaker recognition and speaker verification tasks, this uses conv1D encoder and x-vector based statistics pooling decoder</p>
            </td>
          </tr>

          <tr>
            <!-- <td width="25%">
              <div class="one"><img src='images/proto_1.jpg'></div>
            </td>
             -->
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/1910.11400">
                <papertitle>Meta-learning for robust child-adult classification from speech</papertitle>
              </a>
              <br>
              <strong> Nithin Rao Koluguri</strong>,
              <a href="https://sail.usc.edu/~prabakar/">Manoj Kumar</a>,
              So Hyun Kim, Catherine Lord,
              <a href="https://sail.usc.edu/people/shri.php">Shrikanth Narayanan</a>
              <br>
              <br>
              <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020</em>
              <br>
              <a href="https://github.com/usc-sail/care-proto-child-adult">project page</a>
              <p></p>
              <p>We demonstrate improvements over state-of-the-art speaker embeddings (x-vectors) for speaker classification using prototypical networks</p>
            </td>
          </tr>

          <tr>
            <!-- <td width="25%">
              <div class="one"><img src='images/ALS.png'></div>
            </td> -->
            <td valign="top" width="75%">
              <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1285.pdf">
                <papertitle>Comparison Of Speech Tasks And Recording Devices For Voice Based Automatic Classification Of Healthy Subjects And Patients With Amyotrophic Lateral Sclerosis</papertitle>
              </a>
              <br>
              Suhas B.N,
              Deep Patel,
              <strong> Nithin Rao Koluguri</strong>,
              <a href="http://www.ee.iisc.ac.in/new/people/faculty/prasantg/">Prasanta Ghosh</a>*
              <br>
              <br>
              <em>International Speech Communication Association (Interspeech) </em>, 2019
              <br>
              <a href="http://spire.ee.iisc.ac.in/spire/allPublications.php">project page</a>
              <p></p>
              <p>We evaluated role of different speech tasks and recording devices in detecting ALS through speech</p>
            </td>
          </tr>

          <tr>
            <!-- <td width="25%">
              <div class="one"><img src='images/MWSG.png' width="200" height="120"></div>
            </td> -->
            <td valign="top" width="75%">
              <a href="https://ieeexplore.ieee.org/abstract/document/7933047">
                <papertitle>Spectrogram Enhancement Using Multiple Window Savitzky-Golay (MWSG) Filter for Robust Bird Sound Detection</papertitle>
              </a>
              <br>
              <strong>Nithin Rao Koluguri</strong>,
              <a href="http://www.ee.iisc.ac.in/new/people/students/phd/gnisha/index.html">Nisha G Meenakshi</a>*,
              <a href="http://www.ee.iisc.ac.in/new/people/faculty/prasantg/">Prasanta Ghosh</a>*
              <br>
              <br>
              <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2017
              <br>
              <a href="https://github.com/nithinraok/MWSG_IEEE_Paper">project page</a>
              <p></p>
              <p>We propose a novel unsupervised method to denoise a spectrogram using Multiple Window Savitzky Golay algorithm, and use to enhance bird sound ques to recognize their sounds in noisy environments</p>
            </td>
          </tr>

        </table>

        <!-- Section for Adding Academic Reviewer. and list as 
            \section{\textbf{ACADEMIC REVIEWER}}
 \textbf{Conferences:}
 \resumeItemListStart
   \item 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
   \item 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)
   \item 2023 IEEE International Speech Communication Association (Interspeech)
\resumeItemListEnd
         -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading style="color:rgb(244, 189, 184);" >Academic Reviewer</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="75%" valign="top">
              <p>
                <strong>Conferences:</strong>
                <br>
                <ul>
                  <li>2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</li>
                  <li>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</li>
                  <li>2023 IEEE International Speech Communication Association (Interspeech)</li>
                </ul>
              </p>
            </td>
          </tr>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading style="color:rgb(244, 189, 184);" > Projects</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">

          <tr>
            <!-- <td width="25%"><img src="images/sausage.jpg" width="200" height="120"></td> -->
            <td width="75%" valign="top">
              <p>
                <a href="">
                  <papertitle>Generating sausages from text using WFSTs without acoustic model (DR Project)</papertitle>
                </a>
    <br>
		<strong>Nithin Rao Koluguri</strong>, <a href="">Prashanth G</a>, <a href="https://viterbi.usc.edu/directory/faculty/Georgiou/Panayiotis">Prof. Panayiotis Georgiou,</a>  2019 
		<br>
		<br> Developed a method to mimic an ASR for a given Noise type at certain db level to generate nbest paths from text alone using sausages and confusion matrix. Generated text can be used swiftly to improve an ASR correction model.<br>
              </p>
            </td>
          </tr>
          <tr>
            <!-- <td width="25%"><img src="images/VQA.png" width="200" height="160"></td> -->
            <td width="75%" valign="top">
              <p>
                <a href="https://medium.com/@nithinraok_/visual-question-answering-attention-and-fusion-based-approaches-ebef62fa55aa">
                  <papertitle>Visual Question Answering &#8208 Attention and Fusion based approaches</papertitle>
                </a>
                <br>
		<strong>Nithin Rao Koluguri</strong>, <a href="https://github.com/digbose92">Digbalay Bose</a>,<a href=""> Namrata Tam</a>,  Aditya Mate 2019 
                <br>
                <br> Explored Visual Question Answering methods and looked at applying attention and fusion based methods specifically bottom-up and top-down. Explored BERT Embeddings and different representations for decent VQA system.
                
              </p>
            </td>
          </tr>

	  <tr>
            <!-- <td width="25%"><img src="images/kaldi.png" width="200" height="160"></td> -->
            <td width="75%" valign="top">
              <p>
                <a href="https://github.com/nithinraok/Decision-level-data-fusion-of-Image-and-Speech-recognition-system">
                  <papertitle>Image recognition system based on speech command</papertitle>
                </a>
                <br>
		<strong>Nithin Rao Koluguri</strong>, Nikhil, Gaurav Newalkar, 2016 
                <br>
                  <br> Designed an image recognition system based on the input given through speech. The goal of the project is to assist visual-impaired persons to know the coordinates of the object they are looking for just by their speech. The system on overall achieved an accuracy of 93.75% on speech digits
              </p>
            </td>
	  </tr> 
        </table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading style="color:rgb(244, 189, 184);" >Blogs</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <!-- <td width="25%"><img src="images/Blog.png" width="200" height="160"></td> -->
            <td width="75%" valign="top">
              <p>
                <a href="https://medium.com/@nithinraok_/decoding-an-audio-file-using-a-pre-trained-model-with-kaldi-c1d7d2fe3dc5">
                  <papertitle>Decoding an audio file using a pre-trained model with Kaldi</papertitle>
                </a>
		</p>

		<p>
                <a href="https://medium.com/@nithinraok_/how-to-get-timestamps-of-an-audio-file-using-pre-trained-model-with-kaldi-7b279f59fcde">
                  <papertitle>How to get timestamps of an audio file using pre-trained model with Kaldi</papertitle>
                </a>
              </p>

		<p>
                <a href="https://medium.com/@nithinraok_/visual-question-answering-attention-and-fusion-based-approaches-ebef62fa55aa">
                  <papertitle>Visual Question Answering &#8208 Attention and Fusion based approaches</papertitle>
                </a>
              </p>

            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr><td>
          <br>
            <p align="right"><font size="2">
            <a href="http://www.cs.berkeley.edu/~barron/">inspired from this website</a>
            </font></p>
          </td></tr></tbody>
        </table>
        </td>
    </tr>
  </table>
</body>

</html>
